import chainlit as cl
from chainlit.input_widget import Select, Switch, Slider
from chainlit.sync import run_sync
from openai import AsyncOpenAI
from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
from PIL import Image
import torch
import os
import json
# from minirag import MiniRAG, QueryParam
# from minirag.llm.openai import openai_complete_if_cache, openai_embed
# from minirag.utils import EmbeddingFunc
import requests
import chainlit as cl
from chainlit.data.sql_alchemy import SQLAlchemyDataLayer
from local_storage import MinIOStorageClient
import asyncio
from chain_setup import initialize_chain
#from ocr_utils import process_image_ocr
from file_processor import process_file
import ast
from functions.FunctionManager import FunctionManager
import inspect
import importlib
from openai.types.chat.chat_completion_message import ChatCompletionMessage  # å‡è®¾æ–‡ä»¶åœ¨ç›¸åŒç›®å½•
import re
from playwright.async_api import async_playwright
import urllib.parse
import random
from transformers import BitsAndBytesConfig
from f5_tts.api import F5TTS
from pathlib import Path
import uuid
from modelscope import AutoModel, AutoTokenizer
import torchvision.transforms as T
from transformers import TextIteratorStreamer
from threading import Thread
from torchvision.transforms.functional import InterpolationMode  # å›¾åƒæ’å€¼æ¨¡å¼
from ligrag_function_class import QueryConfig , LightRAGClient
import opencc
from decord import VideoReader, cpu
import numpy as np
from mcp import ClientSession

@cl.data_layer
def get_data_layer():
    storage_provider = MinIOStorageClient(
        endpoint = "localhost:9000",  # MinIO æœåŠ¡å™¨åœ°å€
        access_key = os.environ.get("data_layer_access_key"),
        secret_key = os.environ.get("data_layer_secret_key"),
        bucket_name= "my-bucket",  # Bucket åç§°
    )
    return SQLAlchemyDataLayer(
        conninfo="postgresql+asyncpg://postgres:kobe@localhost:5432/chainlit_db",
        storage_provider=storage_provider
    )

# ======================== tiktoken_cache_diré…ç½®åˆå§‹åŒ– ========================
# ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²é¿å…è½¬ä¹‰é—®é¢˜
tiktoken_cache_dir = r"D:\chainlit\chainlit-datalayer\demo_app\tiktoken_cache"

os.makedirs(tiktoken_cache_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨

cache_file_path = os.path.join(tiktoken_cache_dir, "fb374d419588a4632f3f557e76b4b70aebbca790")

if not os.path.exists(cache_file_path):
    with open(cache_file_path, 'w') as f:
        f.write('')  # åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶

assert os.path.exists(cache_file_path), f"ç¼“å­˜æ–‡ä»¶ {cache_file_path} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–æ‰‹åŠ¨åˆ›å»ºè¯¥æ–‡ä»¶ã€‚"

TF_ENABLE_ONEDNN_OPTS=0
torch.backends.cuda.enable_flash_sdp(False)  # ç¦ç”¨ flash attention

# ======================== æ¨¡å‹é…ç½®åˆå§‹åŒ– ========================
client = AsyncOpenAI(
    base_url= os.environ.get("LLM_BINDING_HOST"),
    api_key = os.environ.get("OPENAI_API_KEY")
)


# ======================== å…¨å±€å˜é‡å£°æ˜ ========================
# åœ¨æ–‡ä»¶å¼€å¤´ï¼ˆå‡½æ•°å¤–éƒ¨ï¼‰å£°æ˜å…¨å±€å˜é‡
image_processor = None
tts = None
image_model = None
is_stop = False
is_tools = False
is_mcp_tools = False
ocr_model = None
ocr_tokenizer = None
num_patches_list = None
video_prefix = ''
MAX_ITER = 100
# é…ç½®é¡¹
TEMP_DIR = Path("tts_temp")
TEMP_DIR.mkdir(exist_ok=True)
valid_modes = ["text_mode", "image_mode", "knowledge_mode", "web_search_mode", "qa_mode", "image_ocr_ai_mode","calling_tools", "voice_mode", "mcp_calling_tools"]

# ======================== commandsæ¶ˆæ¯å¤„ç† ========================
commands = [
    {"id": "file_upload", "icon": "Upload", "description": "ä¸Šä¼ æ–‡ä»¶"},
    {"id": "Tool_reset_memory", "icon": "memory-stick", "description": "å·¥å…·é‡ç½®è®°å¿†"},
    {"id": "Code_Runner", "icon": "square-terminal", "description": "ä»£ç è¿è¡Œå™¨"},
]

# è¯­éŸ³æ¨¡å‹åŠ è½½æ–¹å¼ä¸ºæ‡’åŠ è½½
def get_tts():
    if not hasattr(cl.user_session, "tts"):
        # æ¯ä¸ªç”¨æˆ·ä¼šè¯åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
        cl.user_session.tts = F5TTS(
            model="F5TTS_v1_Base",
            ckpt_file=r"D:\chainlit\models\F5TTS_v1_Base\model_1250000.safetensors"
        )
    return cl.user_session.tts

# ======================== OCRæ¨¡å‹åŠ è½½ ========================
# é…ç½®å‚æ•°
MODEL_NAME = r"D:\chainlit\models\InternVL3-14B-Instruct"
# ======================== OCRæ¨¡å‹åŠ è½½ ========================
def load_ocr_model():
    global ocr_model
    global ocr_tokenizer

    ocr_model = AutoModel.from_pretrained(
        MODEL_NAME,
        #load_in_8bit=True,
        load_in_4bit=True,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        use_flash_attn=True,
        device_map="auto",
        trust_remote_code=True).eval()
    
    
    ocr_tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True
    )
    
    print("OCRæ¨¡å‹å·²å°±ç»ª, è¯·ä¸Šä¼ å›¾ç‰‡å¼€å§‹è¯†åˆ«")


# ======================== MiniRAGåˆå§‹åŒ– ========================
# async def openai_llm_complete(prompt, max_tokens=1024, **kwargs):
#     return await openai_complete_if_cache(
#         base_url=os.environ.get("LLM_BINDING_HOST"),
#         api_key=os.environ.get("OPENAI_API_KEY"),
#         model=os.environ.get("LLM_MODEL"),
#         prompt=prompt,
#         max_tokens=max_tokens,
#         **kwargs
#     )

# async def openai_embedding_func(texts):
#     return await openai_embed(
#         texts=texts,
#         model=os.environ.get("EMBEDDING_MODEL"),
#         base_url=os.environ.get("EMBEDDING_BINDING_HOST"),
#         api_key=os.environ.get("EMBEDDING_BINDING_API_KEY"),
#     )

# rag = MiniRAG(
#     working_dir=r"D:\chainlit\chainlit-datalayer\demo_app\RAG_working_dir",
#     llm_model_func=openai_llm_complete,
#     llm_model_max_token_size=1024,
#     llm_model_name=os.environ.get("LLM_MODEL"),
#     embedding_func=EmbeddingFunc(
#         embedding_dim=int(os.environ.get("EMBEDDING_DIM")),
#         max_token_size=1024,
#         func=openai_embedding_func
#     ),
# )

# ======================== åŠ è½½æ’ä»¶åŠŸèƒ½ ========================
plugin_dirs = [
    d for d in os.listdir('plugins')
    if os.path.isdir(os.path.join('plugins', d)) and d != '__pycache__'
]

functions = []
for dir in plugin_dirs:
    try:
        with open(f'plugins/{dir}/config.json', 'r') as f:
            config = json.load(f)
        enabled = config.get('enabled', True)
    except FileNotFoundError:
        enabled = True

    if not enabled:
        continue

    module = importlib.import_module(f'plugins.{dir}.functions')
    functions.extend([
        obj for name, obj in inspect.getmembers(module) if inspect.isfunction(obj)
    ])

function_manager = FunctionManager(functions=functions)
tools = [{"type": "function", "function": fn} for fn in function_manager.generate_functions_array()]

functions_json = json.dumps(
    [fn["function"] for fn in tools],
    indent=2,
    ensure_ascii=False
)

# ======================== æ¨¡å‹ç³»ç»Ÿä½¿ç”¨æç¤º ========================
language = os.environ.get("SUMMARY_LANGUAGE") or "chinese"
# ä¿®æ”¹ç³»ç»Ÿæç¤ºçš„ç»“æ„ï¼Œæ¯æ¬¡ç”Ÿæˆæ—¶å¼ºåˆ¶åŒ…å«å·¥å…·ä¿¡æ¯
def get_system_message(functions_json , language):
    return f"""
            æ‚¨æ˜¯ä¸€ä¸ªé«˜çº§äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œé€šè¿‡å·¥å…·è°ƒç”¨ä¸ç”¨æˆ·äº¤äº’ã€‚
            å½“å‰å¯ç”¨å·¥å…·ï¼ˆå¿…é¡»ä¸¥æ ¼æŒ‰è§„èŒƒè°ƒç”¨ï¼‰ï¼š
            {functions_json}
            ä½œä¸ºå¼€æ”¾è§£é‡Šå™¨ï¼Œæ‚¨æ˜¯èƒ½æ‰§è¡Œä»£ç å®Œæˆä»»ä½•ç›®æ ‡çš„ä¸–ç•Œçº§ç¨‹åºå‘˜ï¼š
            1. æ‰§è¡Œä»£ç æ—¶æ‹¥æœ‰ç”¨æˆ·æœºå™¨çš„å®Œå…¨æƒé™
            2. å¯ä»¥å®‰è£…æ–°åŒ…ã€è®¿é—®äº’è”ç½‘
            3. é‡åˆ°å¤±è´¥ä¼šè‡ªåŠ¨é‡è¯•
            4. å½“ç”¨æˆ·æåˆ°ä¸€ä¸ªæ–‡ä»¶åæ—¶ï¼ŒæŒ‡çš„æ˜¯æ‚¨å½“å‰ç›®å½•ä¸­ç°æœ‰çš„æ–‡ä»¶ï¼Œå¯ä»¥å½“å‰è·¯å¾„å¤„ç†æ–‡ä»¶ã€‚

            è¯·å§‹ç»ˆæŒ‰ä»¥ä¸‹æµç¨‹å¤„ç†ï¼š
            1. åˆ†æéœ€æ±‚å¹¶åˆ¶å®šè®¡åˆ’ï¼Œå°½é‡ç®€åŒ–è®¡åˆ’çš„æ­¥éª¤ã€‚
            2. é€‰æ‹©åˆé€‚å·¥å…·æˆ–ç›´æ¥æ‰§è¡Œä»£ç ï¼Œä¸è¦è¯•å›¾åœ¨ä¸€ä¸ªä»£ç å—ä¸­å®Œæˆæ‰€æœ‰äº‹æƒ…ï¼Œè¿™ä¸€ç‚¹è‡³å…³é‡è¦ã€‚
            æ‚¨åº”è¯¥å°è¯•æŸä»¶äº‹ï¼Œæ‰“å°æœ‰å…³å®ƒçš„ä¿¡æ¯ï¼Œç„¶åä»é‚£é‡Œç»§ç»­è¿›è¡Œå¾®å°çš„ã€æ˜æ™ºçš„æ­¥éª¤ã€‚
            æ‚¨æ°¸è¿œä¸ä¼šåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶å°±æˆåŠŸï¼Œè€Œå°è¯•ä¸€æ¬¡å®Œæˆæ‰€æœ‰äº‹æƒ…é€šå¸¸ä¼šå¯¼è‡´æ‚¨çœ‹ä¸åˆ°çš„é”™è¯¯ã€‚
            3. ä¸¥æ ¼ä½¿ç”¨åˆæ³•JSONæ ¼å¼è°ƒç”¨å·¥å…·å‚æ•°
            è¯·ä½¿ç”¨{language}äº¤æµ
            """
system_message = get_system_message(functions_json , language)  # æ¯æ¬¡ç”Ÿæˆæ—¶é‡æ–°è·å–


# ======================== æŸ¥çœ‹æœ‰æ‰€æœ‰å‡½æ•°ä¸ç›¸å…³å‚æ•° ========================
# å°† tools è½¬æ¢ä¸º JSON æ ¼å¼
tools_json = json.dumps(tools, indent=4)
# æ‰“å° JSON æ ¼å¼çš„ tools
#print ("tools_json:\n",tools_json)


# ======================== è®¾ç½®é»˜è®¤å·¥ä½œç›®å½• ========================
# æ‰“å°å½“å‰ç›®å½•
#print("å½“å‰ç›®å½•:", os.getcwd())
# è®¾ç½®é»˜è®¤ä¿å­˜è·¯å¾„
os.chdir(os.getcwd())

# ======================== ç”¨æˆ·è®¤è¯ ========================
@cl.password_auth_callback
def auth_callback(username: str, password: str):
    if (username, password) == ("admin", "admin"):
        return cl.User(
            identifier="admin", 
            metadata={"role": "admin", "provider": "credentials"}
        )
    return None

# ======================== on_chat_start ========================
@cl.on_chat_start
async def on_chat_start():
    """èŠå¤©å¼€å§‹æ—¶åˆå§‹åŒ–è®¾ç½®"""
    # åˆå§‹åŒ–é»˜è®¤ä¼šè¯æ•°æ®

    cl.user_session.set("message_history", [{
        "role": "system",
        "content": system_message
    }])

    default_mode = "text_mode"
    cl.user_session.set("processing_mode", default_mode)
    cl.user_session.set("conversation_history", [])
    cl.user_session.set("resume_data", None)
    current_mode = default_mode
    await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
    

# ======================== on_chat_resume ========================
@cl.on_chat_resume
async def on_chat_resume(conversation: dict):

    cl.user_session.set("resume_data", None)  
    cl.user_session.set("resume_data", conversation)
    session_id = conversation.get("id")
    print(f"æ¢å¤ä¼šè¯ID: {session_id}")
    if resume_data := load_resume_data(session_id):
        saved_mode = resume_data.get("processing_mode")
        
        current_mode = saved_mode if saved_mode in valid_modes else "text_mode"
        if current_mode == "calling_tools":
            await cl.context.emitter.set_commands(commands)
        else:
            await cl.context.emitter.set_commands([])

        await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿

def load_resume_data(session_id: str):
    save_dir = os.path.join(os.path.dirname(__file__), "conversation")  # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    file_path = os.path.join(save_dir, f"conversation_{session_id}.json")
    try:
        with open(file_path, "r") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return None
         
# ======================== on_chat_end ========================
@cl.on_chat_end
async def on_chat_end():
    """ä¼šè¯ç»“æŸè‡ªåŠ¨ä¿å­˜"""
    session_id = None
    conversation = cl.user_session.get("resume_data")
    if conversation:
        session_id = conversation.get("id")
        print(f"ä¼šè¯ç»“æŸè‡ªåŠ¨ä¿å­˜ID: {session_id}")
    #print (f"ä¼šè¯ç»“æŸè‡ªåŠ¨ä¿å­˜æ•°æ®: {conversation}")
    if session_id:
        save_resume_data(session_id, {
            "resume_data": cl.user_session.get("resume_data"),
            "processing_mode": cl.user_session.get("processing_mode")  # æ–°å¢æ¨¡å¼ä¿å­˜
        })
    cl.user_session.set("resume_data", None)  # æ¸…é™¤ä¼šè¯æ•°æ®


def save_resume_data(session_id: str, data: dict):
    save_dir = os.path.join(os.path.dirname(__file__), "conversation")  # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    file_path = os.path.join(save_dir, f"conversation_{session_id}.json")
    # Save the data
    with open(file_path, "w") as f:
        json.dump(data, f)


# ======================== on_stop ========================
@cl.on_stop
async def stop_chat():
    global is_stop
    is_stop = True
    print("ä¼šè¯å·²ç»ˆæ­¢")


# ======================== on_message_toolsæ¶ˆæ¯å¤„ç† ========================
async def on_message_tools(message: cl.Message):
    global is_stop
    cur_iter = 0
    tool_call_id = True
    user_message = message.content
    cl.user_session.set("user_message", user_message)
    message_history = cl.user_session.get("message_history", [])
    message_history.append({"role": "user", "content": user_message})

    # æ¶ˆæ¯å†å²ç®¡ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    if len(message_history) > 20:
        message_history = [message_history[0]] + message_history[-39:]

    # å‘½ä»¤å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    if message.command == "file_upload":
        file_upload_user_message = cl.user_session.set("user_message", "ä¸Šè½½æ–‡ä»¶")
        result = await function_manager.call_function("need_file_upload",  {"user_message": file_upload_user_message})
        #message_history.append({"role": "assistant", "content": result})
        message_history = [{
            "role": "system", 
            "content": get_system_message(functions_json, language)
        }]
        await cl.Message(content=f"å¤„ç†ç»“æœ:\n{result}", language="json").send()
        tool_call_id = await tool_calls(message_history)


    if message.command == "Tool_reset_memory":
        message_history = [{
            "role": "system", 
            "content": get_system_message(functions_json, language)
        }]
        await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
        await cl.Message(content="è®°å¿†å·²é‡ç½®").send()

    # å‘½ä»¤å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    if message.command == "Code_Runner":
        #user_message = f"```py\n{user_message}\n```"
        await cl.Message(content=user_message , language="python").send()
        result = await function_manager.call_function("python_exec", {"code": user_message})
        message_history.append({"role": "assistant", "content": result})
        await cl.Message(content=f"å¤„ç†ç»“æœ:\n{result}" , language="json").send()
        tool_call_id = await tool_calls(message_history)

    while cur_iter < MAX_ITER and not is_stop:
        
        # æ¯æ¬¡è¯·æ±‚å‰é‡æ–°æ³¨å…¥ç³»ç»Ÿæç¤º
        current_system_message = get_system_message(functions_json, language)
        
        # æ£€æŸ¥ enhanced_history æ˜¯å¦å·²ç»åŒ…å«ç³»ç»Ÿæ¶ˆæ¯
        system_message_exists = any(
            msg["role"] == "system" and msg["content"] == current_system_message
            for msg in message_history
        )
        
        # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåˆ™æ·»åŠ 
        if not system_message_exists:
            enhanced_history = [
                {"role": "system", "content": current_system_message},
                *[msg for msg in message_history if msg["role"] != "system"]
            ]
        else:
            enhanced_history = message_history

        if is_stop:
            is_stop = False
            break

        #å¦‚æœ tool_call_id æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œåˆ™ç»ˆæ­¢å¾ªç¯ã€‚
        if not tool_call_id:
            break

        #print (enhanced_history)
        tool_call_id = await tool_calls(enhanced_history)
        
        cur_iter += 1
        continue


# ======================== mainæ¶ˆæ¯å¤„ç† ========================
async def main(message: cl.Message):
    # è·å–å½“å‰æ¨¡å¼
    current_mode = cl.user_session.get("processing_mode")
    print(f"å½“å‰æ¨¡å¼: {current_mode}")  # æ·»åŠ è°ƒè¯•è¾“å‡º
    global is_tools
    global is_mcp_tools

    # æ¨¡å¼æœ‰æ•ˆæ€§éªŒè¯
    if current_mode not in valid_modes:
        await cl.Message("âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆæ¨¡å¼ï¼Œå·²é‡ç½®ä¸ºæ–‡æœ¬å¤„ç†").send()
        current_mode = "text_mode"
        cl.user_session.set("processing_mode", current_mode)
    
    if current_mode != "calling_tools":  
        is_tools = False
    else:
        is_tools = True

    if current_mode != "mcp_calling_tools":  
        is_mcp_tools = False
    else:
        is_mcp_tools = True

    # æ ¹æ®æ¨¡å¼å¤„ç†æ¶ˆæ¯
    if current_mode == "image_mode":
        # å›¾ç‰‡å¤„ç†é€»è¾‘
        if not message.elements or not message.elements[0].mime.startswith("image"):
            await cl.Message("âŒ å›¾ç‰‡æ¨¡å¼éœ€è¦ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶").send()
            return
        #await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        await handle_image_input(message, cl.user_session.get("conversation_history", []))


    elif current_mode == "knowledge_mode":
        # çŸ¥è¯†åº“å¤„ç†é€»è¾‘
        #await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        await handle_post_upload_actions(message.content)

    
    elif current_mode == "web_search_mode":
        # è”ç½‘æœç´¢å¤„ç†é€»è¾‘
        #await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        await handle_web_search(message.content)

    
    elif current_mode == "image_ocr_ai_mode":
        # å›¾ç‰‡OCRå¤„ç†é€»è¾‘
        await handle_image_ai_ocr(message)

    # elif current_mode == "image_ocr_mode":
    #     # å›¾ç‰‡OCRå¤„ç†é€»è¾‘
    #     #await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
    #     await handle_image_ocr(message)


    elif current_mode == "qa_mode":
        # æ–‡ä»¶é—®ç­”å¤„ç†é€»è¾‘
        #await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        await qa_text_input(message.content)


    elif current_mode == "voice_mode":
        # è¯­éŸ³é—®ç­”å¤„ç†é€»è¾‘
        #await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        await handle_voice_mode(message)

    else:
        # é»˜è®¤æ–‡æœ¬å¤„ç†
        #await update_mode_selector(current_mode)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        await handle_text_input(
            cl.user_session.get("conversation_history", []),
            message.content
        )
        
    cl.user_session.set("processing_mode", current_mode)
    # è·å–å½“å‰å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯
    context = cl.chat_context.to_openai()
    #print(context)  # æ‰“å°å¯¹è¯ä¸Šä¸‹æ–‡
    cl.user_session.set("conversation_history", context) # ä¿å­˜å¯¹è¯å†å²   


# ======================== ä¸»è¦è¿è¡Œchainlitæ¶ˆæ¯å¤„ç† ========================
@cl.on_message
async def run_conversation(message: cl.Message):
    global is_tools
    global is_mcp_tools
    if is_tools:
        await on_message_tools(message)

    print ("is_mcp_tools:\n",is_mcp_tools)

    if is_mcp_tools:
        await on_message_mcp_tools(message)
    else:
        await main(message)


# ======================== ChatSettingsè®¾ç½®æ›´æ–°æ—¶è¢«è°ƒç”¨ ========================
@cl.on_settings_update
async def setup_agent(settings):
    # å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡
    global image_processor
    global image_model
    global is_tools
    global is_mcp_tools
    global tts
    global ocr_model
    global ocr_tokenizer

    print("Setup agent with following settings: ", settings)
    # Setup agent with following settings:  {'mode_selector': 'knowledge_mode'}
    # ä»settingså­—å…¸ä¸­è·å–mode_selectorçš„å€¼
    mode_value = settings.get("mode_selector", "text_mode")  # ç¬¬äºŒä¸ªå‚æ•°æ˜¯å¯é€‰çš„é»˜è®¤å€¼
    if settings.get("describe") == None:  #å¯¹åº”çš„å€¼æ˜¯å¦ä¸ä¸ºç©º
        await cl.Message(content=f"å¤„ç†æ¨¡å¼ä¸º: {mode_value}").send()
    cl.user_session.set("processing_mode", mode_value)

    if mode_value != "mcp_calling_tools":  
        is_mcp_tools = False
    else:
        is_mcp_tools = True

    if mode_value != "calling_tools":  
        await cl.context.emitter.set_commands([])
        is_tools = False
    else:
        await cl.context.emitter.set_commands(commands)
        is_tools = True

    if mode_value != "image_ocr_ai_mode":  # å¦‚æœä¸æ˜¯ aiå›¾ç‰‡ocr
        await update_mode_selector(mode_value)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        if ocr_tokenizer is not None and ocr_model is not None:
            model_msg = cl.Message(content="æ­£åœ¨å¸è½½ocræ¨¡å‹...")
            await model_msg.send()
            await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
            unload_ocr_model()  # è°ƒç”¨å¸è½½å‡½æ•°
            model_msg.content = "å¸è½½ocræ¨¡å‹æˆåŠŸ"
            await model_msg.update()
            ocr_tokenizer = None
            ocr_model = None

    if mode_value != "voice_mode":  # å¦‚æœä¸æ˜¯ voice_mode
        if tts is not None:
            model_msg = cl.Message(content="æ­£åœ¨å¸è½½è¯­éŸ³æ¨¡å‹...")
            await model_msg.send()
            await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
            unload_voice_model()
            model_msg.content = "å¸è½½è¯­éŸ³æ¨¡å‹æˆåŠŸ"
            await model_msg.update()
            tts = None


    if mode_value != "image_mode":  # å¦‚æœä¸æ˜¯ image_mode
        if image_processor is not None and image_model is not None:
            model_msg = cl.Message(content="æ­£åœ¨å¸è½½å›¾ç‰‡æ¨¡å‹...")
            await model_msg.send()
            await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
            unload_image_model_and_processor(image_processor, image_model)  # å¸è½½å›¾ç‰‡æ¨¡å‹
            model_msg.content = "å¸è½½å›¾ç‰‡æ¨¡å‹æˆåŠŸ"
            await model_msg.update()
            image_processor = None
            image_model = None        

    if mode_value == "qa_mode":

        model_msg = cl.Message(content="æ­£åœ¨å‡†å¤‡...")
        await model_msg.send()
        await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°

        # åŠ è½½æ¨¡å‹
        tts = get_tts()  # æ›¿æ¢åŸæ¥çš„å…¨å±€å¼•ç”¨

        await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
        model_msg.content = "å‡†å¤‡æˆåŠŸ"
        await model_msg.update()

        files = None
        # åœ¨ä»£ç å¼€å§‹å¤„è®¾ç½® chain ä¸ºç©ºå€¼
        cl.user_session.set("chain", None)
        # ç­‰å¾…ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
        while files is None:
            files = await cl.AskFileMessage(
                content="è¯·ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶ä»¥å¼€å§‹ï¼",  # æç¤ºç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
                accept=["*/*"],  # æ¥å—ä»»ä½•æ–‡ä»¶ç±»å‹
                max_size_mb=100,  # æ–‡ä»¶æœ€å¤§å¤§å°ä¸º20MB
                timeout=300,  # è¶…æ—¶æ—¶é—´ä¸º180ç§’
            ).send()
        file_path = files[-1].path  # è·å–ç”¨æˆ·ä¸Šä¼ çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶è·¯å¾„
        file_name = files[-1].name  # è·å–ç”¨æˆ·ä¸Šä¼ çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶å
        print (file_name)
        # æ¸…ç† Chroma çš„é»˜è®¤æŒä¹…åŒ–æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists("./chroma"):
            import shutil
            shutil.rmtree("./chroma")
        async def process_file_qa_mode(file):
            # åˆå§‹åŒ–å¯¹è¯é“¾
            chain = initialize_chain(file)
            return chain
        
        await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
        # å‘é€æ¶ˆæ¯ï¼Œæç¤ºæ­£åœ¨å¤„ç†æ–‡ä»¶
        msg = cl.Message(content=f"æ­£åœ¨å¤„ç† `{file_name}`...")
        await msg.send()
        await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°

        if file_name:
            if '.wav' in file_name:
                ref_text = tts.transcribe(file_path) 
                converter = opencc.OpenCC('t2s')
                ref_text = converter.convert(ref_text)
                msg.content = ref_text
            else:
                # ä½¿ç”¨å¼‚æ­¥çº¿ç¨‹æ‰§è¡Œé˜»å¡æ“ä½œ
                loop = asyncio.get_event_loop()
                chain = await loop.run_in_executor(None, lambda: asyncio.run(process_file_qa_mode(file_path)))
                # é€šçŸ¥ç”¨æˆ·ç³»ç»Ÿå·²å‡†å¤‡å¥½
                msg.content = f"å¤„ç† `{file_name}` å®Œæˆã€‚ä½ ç°åœ¨å¯ä»¥æé—®äº†ï¼"
                # å°†å¯¹è¯é“¾å­˜å‚¨åœ¨ç”¨æˆ·ä¼šè¯ä¸­
                cl.user_session.set("chain", chain)
                
        await msg.update()


    if mode_value == "image_ocr_ai_mode":
        await update_image_ocr_ai_mode_selector(mode_value)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        print (settings.get("describe"))
        if settings.get("describe") == True:
            cl.user_session.set("image_describe", "è¯·ç”¨ä¸€æ®µè‡ªç„¶è¯­è¨€çš„å¥å­è¯¦ç»†æè¿°å›¾ç‰‡ï¼Œæ‰€æœ‰ç‰©å“éœ€è¦æ˜ç¡®æ•°é‡ï¼Œå¹¶è¯´æ˜ç›¸å…³åœºæ™¯å…ƒç´ ä½äºåœºæ™¯å›¾ç‰‡çš„é‚£ä¸ªæ–¹ä½ã€‚")
            await update_image_ocr_ai_mode_selector(mode_value , True)
        elif settings.get("describe") == False:
            cl.user_session.set("image_describe", "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—, ä¸è¦æ·»åŠ é¢å¤–æ–‡å­—å¦‚å›¾ä¸­æ‰€æœ‰æ–‡å­—ä¹‹ç±»ï¼Œç›´æ¥è¾“å‡ºåŸæ–‡æ–‡å­—ã€‚")
            await update_image_ocr_ai_mode_selector(mode_value , False)
        elif settings.get("describe") == None:
            model_msg = cl.Message(content="æ­£åœ¨åŠ è½½ocræ¨¡å‹...")
            await model_msg.send()
            await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
            # åŠ è½½æ¨¡å‹
            load_ocr_model()  # æ›¿æ¢åŸæ¥çš„å…¨å±€å¼•ç”¨
            await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
            model_msg.content = "ocræ¨¡å‹åŠ è½½æˆåŠŸ"
            await model_msg.update()
            cl.user_session.set("image_describe", "è¯·ç”¨ä¸€æ®µè‡ªç„¶è¯­è¨€çš„å¥å­è¯¦ç»†æè¿°å›¾ç‰‡ï¼Œæ‰€æœ‰ç‰©å“éœ€è¦æ˜ç¡®æ•°é‡ï¼Œå¹¶è¯´æ˜ç›¸å…³åœºæ™¯å…ƒç´ ä½äºåœºæ™¯å›¾ç‰‡çš„é‚£ä¸ªæ–¹ä½ã€‚")
  
    if mode_value == "voice_mode":
        await update_voice_mode_selector(mode_value)  # åŒæ­¥æ›´æ–°è®¾ç½®é¢æ¿
        print (settings.get("voice_mode_describe"))
        if settings.get("voice_mode_describe") == True:
            cl.user_session.set("voice_mode_response", "å¼€å¯æ¨¡å‹è¯­éŸ³å›å¤")
            await update_voice_mode_selector(mode_value , True)
        elif settings.get("voice_mode_describe") == False:
            cl.user_session.set("voice_mode_response", "")  # å…³é—­æ ¹æ®è¾“å…¥æ¡†æ¶ˆæ¯ç”Ÿæˆè¯­éŸ³
            await update_voice_mode_selector(mode_value , False)
        elif settings.get("voice_mode_describe") == None:
            model_msg = cl.Message(content="æ­£åœ¨åŠ è½½è¯­éŸ³æ¨¡å‹...")
            await model_msg.send()
            await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
            # åŠ è½½æ¨¡å‹
            tts = get_tts()  # æ›¿æ¢åŸæ¥çš„å…¨å±€å¼•ç”¨
            await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
            model_msg.content = "è¯­éŸ³æ¨¡å‹åŠ è½½æˆåŠŸ"
            await model_msg.update()
            cl.user_session.set("voice_mode_response", "å¼€å¯æ¨¡å‹è¯­éŸ³å›å¤")

    if mode_value == "image_mode":
        # ç›´æ¥åœ¨ä¸»çº¿ç¨‹ä¸­å‘é€æ¶ˆæ¯
        model_msg = cl.Message(content="æ­£åœ¨åŠ è½½å›¾ç‰‡æ¨¡å‹...")
        await model_msg.send()
        await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°

        # åŠ è½½æ¨¡å‹
        image_processor = AutoProcessor.from_pretrained(
            'D:\\chainlit\\models\\molmo-7B-D-bnb-4bit',
            trust_remote_code=True,
            torch_dtype='auto',
            use_fast=True,
            device_map='auto'
        )
        image_model = AutoModelForCausalLM.from_pretrained(
            'D:\\chainlit\\models\\molmo-7B-D-bnb-4bit',
            trust_remote_code=True,
            torch_dtype='auto',
            device_map='auto',
            load_in_4bit=True,
        )
        await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
        model_msg.content = "å›¾ç‰‡æ¨¡å‹åŠ è½½æˆåŠŸ"
        await model_msg.update()


# ======================== å¸è½½è¯­éŸ³æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œé‡Šæ”¾å†…å­˜ ========================
def unload_voice_model():
    try:
        # è·å–ç”¨æˆ·ä¼šè¯ä¸­çš„æ¨¡å‹å®ä¾‹
        if hasattr(cl.user_session, "tts"):
            tts_instance = cl.user_session.tts
            
            # å…ˆé‡Šæ”¾æ¨¡å‹å†…éƒ¨ç»„ä»¶
            if hasattr(tts_instance, "ema_model"):
                # å°†æ¨¡å‹ç§»å›CPU
                if hasattr(tts_instance.ema_model, "to"):
                    tts_instance.ema_model.to("cpu")
                del tts_instance.ema_model
                
            if hasattr(tts_instance, "vocoder"):
                # é‡Šæ”¾å£°ç å™¨èµ„æº
                if hasattr(tts_instance.vocoder, "to"):
                    tts_instance.vocoder.to("cpu")
                del tts_instance.vocoder
            
            # åˆ é™¤ç”¨æˆ·ä¼šè¯ä¸­çš„æ¨¡å‹å¼•ç”¨
            del cl.user_session.tts
            
            # åŒé‡åƒåœ¾å›æ”¶æœºåˆ¶
            import gc
            for _ in range(3):  # ä¸‰æ¬¡å›æ”¶ç¡®ä¿å½»åº•
                gc.collect()
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                
            print("âœ… è¯­éŸ³æ¨¡å‹å·²æˆåŠŸå¸è½½")
            
    except Exception as e:
        print(f"âŒ å¸è½½è¯­éŸ³æ¨¡å‹å¤±è´¥: {str(e)}")
    finally:
        # ç¡®ä¿ä¼šè¯å±æ€§æ¸…é™¤
        if hasattr(cl.user_session, "tts"):
            del cl.user_session.tts

# ======================== å¸è½½OCRæ¨¡å‹å’Œå¤„ç†å™¨ï¼Œé‡Šæ”¾å†…å­˜ ========================
def unload_ocr_model():
    global ocr_model
    global ocr_tokenizer
    
    cl.user_session.set("last_one_msg_elements", None)
    
    try:
        # åˆ é™¤æ¨¡å‹å’Œåˆ†è¯å™¨çš„æ‰€æœ‰å¼•ç”¨ï¼ˆä¸å†å°è¯•ç§»åŠ¨è®¾å¤‡ï¼‰
        del ocr_model
        del ocr_tokenizer
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            
        print("âœ… OCRæ¨¡å‹å·²æˆåŠŸå¸è½½")
        
    except Exception as e:
        print(f"âŒ å¸è½½OCRæ¨¡å‹å¤±è´¥: {str(e)}")
    finally:
        # ç¡®ä¿å…¨å±€å˜é‡ç½®ç©º
        ocr_model = None
        ocr_tokenizer = None


# ======================== å¸è½½å›¾ç‰‡æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œé‡Šæ”¾å†…å­˜ ========================
def unload_image_model_and_processor(image_processor, image_model):
    """
    å¸è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œé‡Šæ”¾å†…å­˜
    :param image_processor: å·²åŠ è½½çš„å¤„ç†å™¨å¯¹è±¡
    :param model: å·²åŠ è½½çš„æ¨¡å‹å¯¹è±¡
    """

    cl.user_session.set("last_one_msg_elements", None)

    try:
        # ç¡®ä¿æ¨¡å‹å’Œå¤„ç†å™¨åœ¨GPUä¸Š
        if hasattr(image_model, 'device'):
            device = str(image_model.device)
            if 'cuda' in device:
                # å°†æ¨¡å‹ç§»å›CPU
                image_model.to('cpu')
        
        # åˆ é™¤æ¨¡å‹çš„æ‰€æœ‰å¼•ç”¨
        if image_model is not None:
            # æ¸…é™¤æ¨¡å‹å‚æ•°
            for param in image_model.parameters():
                if param is not None:
                    del param
            # æ¸…é™¤æ¨¡å‹ç¼“å†²åŒº
            for buffer in image_model.buffers():
                if buffer is not None:
                    del buffer
            # åˆ é™¤æ¨¡å‹æœ¬èº«
            del image_model
        
        # åˆ é™¤å¤„ç†å™¨
        if image_processor is not None:
            del image_processor
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # æ¸…ç†GPUç¼“å­˜
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()  # é¢å¤–çš„æ¸…ç†
        
        print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨å·²æˆåŠŸå¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")
        
    except Exception as e:
        print(f"âŒ å¸è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")


# ======================== æ¨¡å¼é€‰æ‹©å›è°ƒ ========================
async def update_mode_selector(mode: str):
    """æ›´æ–°è®¾ç½®é¢æ¿çš„Selectç»„ä»¶æ˜¾ç¤º"""
    await cl.ChatSettings(
        [
            Select(
                id="mode_selector",
                label="é€‰æ‹©å¤„ç†æ¨¡å¼",
                items={
                    "æ–‡æœ¬å¯¹è¯": "text_mode",
                    "å›¾ç‰‡åˆ†æ": "image_mode",
                    "çŸ¥è¯†åº“æŸ¥è¯¢": "knowledge_mode",
                    "è”ç½‘æœç´¢": "web_search_mode",
                    "æ–‡ä»¶é—®ç­”": "qa_mode",
                    "AIå›¾ç‰‡OCR": "image_ocr_ai_mode",
                    "è°ƒç”¨toolså·¥å…·": "calling_tools",
                    "è¯­éŸ³å›å¤": "voice_mode",
                    "è°ƒç”¨MCPå·¥å…·": "mcp_calling_tools",
                },
                initial_value=mode,
            ),
        ]
    ).send()


async def update_image_ocr_ai_mode_selector(mode: str,initial=True):
    """æ›´æ–°è®¾ç½®é¢æ¿çš„Selectç»„ä»¶æ˜¾ç¤º"""
    await cl.ChatSettings(
        [
            Select(
                id="mode_selector",
                label="é€‰æ‹©å¤„ç†æ¨¡å¼",
                items={
                    "æ–‡æœ¬å¯¹è¯": "text_mode",
                    "å›¾ç‰‡åˆ†æ": "image_mode",
                    "çŸ¥è¯†åº“æŸ¥è¯¢": "knowledge_mode",
                    "è”ç½‘æœç´¢": "web_search_mode",
                    "æ–‡ä»¶é—®ç­”": "qa_mode",
                    "AIå›¾ç‰‡OCR": "image_ocr_ai_mode",
                    "è°ƒç”¨toolså·¥å…·": "calling_tools",
                    "è¯­éŸ³å›å¤": "voice_mode",
                    "è°ƒç”¨MCPå·¥å…·": "mcp_calling_tools",
                },
                initial_value=mode,
            ),
            Switch(id="describe", label="é»˜è®¤å¼€å¯è¯¦ç»†æè¿°, å…³é—­å¹¶ä¸”è¾“å…¥ä¿¡æ¯å°‘äº2ä¸ªå­—ç¬¦åˆ™åªä¼šOCRå›¾ç‰‡æ–‡å­—", initial=initial),
        ]
    ).send()

async def update_voice_mode_selector(mode: str,initial=True):
    """æ›´æ–°è®¾ç½®é¢æ¿çš„Selectç»„ä»¶æ˜¾ç¤º"""
    await cl.ChatSettings(
        [
            Select(
                id="mode_selector",
                label="é€‰æ‹©å¤„ç†æ¨¡å¼",
                items={
                    "æ–‡æœ¬å¯¹è¯": "text_mode",
                    "å›¾ç‰‡åˆ†æ": "image_mode",
                    "çŸ¥è¯†åº“æŸ¥è¯¢": "knowledge_mode",
                    "è”ç½‘æœç´¢": "web_search_mode",
                    "æ–‡ä»¶é—®ç­”": "qa_mode",
                    "AIå›¾ç‰‡OCR": "image_ocr_ai_mode",
                    "è°ƒç”¨toolså·¥å…·": "calling_tools",
                    "è¯­éŸ³å›å¤": "voice_mode",
                    "è°ƒç”¨MCPå·¥å…·": "mcp_calling_tools",
                },
                initial_value=mode,
            ),
            Switch(id="voice_mode_describe", label="é»˜è®¤å¼€å¯æ¨¡å‹è¯­éŸ³å›å¤, å…³é—­åˆ™æ ¹æ®è¾“å…¥æ¡†æ¶ˆæ¯ç”Ÿæˆè¯­éŸ³", initial=initial),
        ]
    ).send()



# ======================== MCPå·¥å…·è°ƒç”¨å¤„ç† ========================

# å½“ MCP è¿æ¥æ—¶è§¦å‘çš„å¼‚æ­¥å‡½æ•°
@cl.on_mcp_connect
async def on_mcp(connection, session: ClientSession):
    # è·å–å½“å‰ MCP è¿æ¥çš„æ‰€æœ‰å¯ç”¨å·¥å…·
    result = await session.list_tools()
    # æ•´ç†å·¥å…·ä¿¡æ¯ï¼Œå°†å…¶è½¬æ¢ä¸ºå­—å…¸å½¢å¼
    tools = [{
        "name": t.name,
        "description": t.description,
        "input_schema": t.inputSchema,
        } for t in result.tools]
    
    # ä»ç”¨æˆ·ä¼šè¯ä¸­è·å–å·²æœ‰çš„ MCP å·¥å…·åˆ—è¡¨
    mcp_tools = cl.user_session.get("mcp_tools", {})
    # å°†å½“å‰è¿æ¥çš„å·¥å…·æ·»åŠ åˆ° MCP å·¥å…·åˆ—è¡¨ä¸­
    mcp_tools[connection.name] = tools
    # æ›´æ–°ç”¨æˆ·ä¼šè¯ä¸­çš„ MCP å·¥å…·åˆ—è¡¨
    cl.user_session.set("mcp_tools", mcp_tools)


# 2. å¤„ç†MCPè¿æ¥æ–­å¼€
@cl.on_mcp_disconnect
async def on_mcp_disconnect(name: str, session: ClientSession):
    await cl.Message(content=f"MCPæœåŠ¡å™¨ '{name}' å·²æ–­å¼€è¿æ¥").send()


# å·¥å…·è°ƒç”¨æ­¥éª¤çš„å¼‚æ­¥å‡½æ•°
@cl.step(type="tool") 
async def call_mcp_tool(tool_name, tool_input):
    """è°ƒç”¨MCPå·¥å…·å¹¶è¿”å›ç»“æœ"""
    # è·å–å½“å‰æ­¥éª¤ç”¨äºè·Ÿè¸ªå·¥å…·è°ƒç”¨
    current_step = cl.context.current_step
    current_step.name = tool_name
    
    # æŸ¥æ‰¾å·¥å…·æ‰€åœ¨çš„MCPè¿æ¥
    mcp_tools = cl.user_session.get("mcp_tools", {})
    mcp_name = None
    for connection_name, tools in mcp_tools.items():
        if any(tool.get("name") == tool_name for tool in tools):
            mcp_name = connection_name
            break
    
    # é”™è¯¯å¤„ç†ï¼šæœªæ‰¾åˆ°å·¥å…·
    if not mcp_name:
        error_msg = f"å·¥å…· {tool_name} æœªåœ¨ä»»ä½•MCPè¿æ¥ä¸­æ‰¾åˆ°"
        current_step.output = json.dumps({"error": error_msg})
        return error_msg
    
    # è·å–MCPä¼šè¯
    mcp_session, _ = cl.context.session.mcp_sessions.get(mcp_name)
    if not mcp_session:
        error_msg = f"MCPè¿æ¥ {mcp_name} æœªæ‰¾åˆ°"
        current_step.output = json.dumps({"error": error_msg})
        return error_msg
    
    try:
        # å®é™…è°ƒç”¨MCPå·¥å…·
        result = await mcp_session.call_tool(tool_name, tool_input)
        current_step.output = result
        return result
    except Exception as e:
        error_msg = f"è°ƒç”¨å·¥å…· {tool_name} æ—¶å‡ºé”™: {str(e)}"
        current_step.output = json.dumps({"error": error_msg})
        return error_msg



# ======================== on_message_mcp_toolsæ¶ˆæ¯å¤„ç† ========================
async def on_message_mcp_tools(message: cl.Message):

    cur_iter = 0
    tool_call_id = True
    # è·å–ç”¨æˆ·ä¼šè¯ä¸­çš„ MCP å·¥å…·å­—å…¸
    mcp_tools_dict = cl.user_session.get("mcp_tools", {})

    # åˆå¹¶æ‰€æœ‰è¿æ¥çš„å·¥å…·åˆ—è¡¨
    all_tools = []
    for tools_list in mcp_tools_dict.values():
        all_tools.extend(tools_list)


    # è½¬æ¢å·¥å…·æ ¼å¼ä»¥é€‚åº”OpenAIçš„å‡½æ•°è°ƒç”¨è¦æ±‚
    openai_tools = []
    for tool in all_tools:  # ç°åœ¨toolæ˜¯å­—å…¸
        openai_tool = {
            "type": "function",
            "function": {
                "name": tool["name"],
                "description": tool["description"],
                "parameters": tool["input_schema"]
            }
        }
        openai_tools.append(openai_tool)

    # æ‰“å°å·¥å…·åç§°åˆ—è¡¨ï¼Œæ–¹ä¾¿è°ƒè¯•
    print("openai_tools:\n",openai_tools)

    user_message = message.content
    cl.user_session.set("user_message", user_message)
    message_history = cl.user_session.get("message_history", [])
    message_history.append({"role": "user", "content": user_message})

    # æ¶ˆæ¯å†å²ç®¡ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    if len(message_history) > 20:
        message_history = [message_history[0]] + message_history[-39:]

    while cur_iter < MAX_ITER:
        # æ¯æ¬¡è¯·æ±‚å‰é‡æ–°æ³¨å…¥ç³»ç»Ÿæç¤º
        current_system_message = get_system_message(openai_tools, language)
        
        # æ£€æŸ¥ enhanced_history æ˜¯å¦å·²ç»åŒ…å«ç³»ç»Ÿæ¶ˆæ¯
        system_message_exists = any(
            msg["role"] == "system" and msg["content"] == current_system_message
            for msg in message_history
        )
        
        # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåˆ™æ·»åŠ 
        if not system_message_exists:
            enhanced_history = [
                {"role": "system", "content": current_system_message},
                *[msg for msg in message_history if msg["role"] != "system"]
            ]
        else:
            enhanced_history = message_history

        #å¦‚æœ tool_call_id æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œåˆ™ç»ˆæ­¢å¾ªç¯ã€‚
        if not tool_call_id:
            break

        #print (enhanced_history)
        tool_call_id = await tool_mcp_calls(enhanced_history, openai_tools)

        cur_iter += 1
        continue


# ======================== tool_mcp_callsæµå¼è¯·æ±‚å¤„ç† ========================
async def tool_mcp_calls(message_history: list, tools:any):
    # å·¥å…·è°ƒç”¨å¤„ç†ï¼ˆéæµå¼ï¼‰
    full_resp = await client.chat.completions.create(
        model=os.environ.get("LLM_MODEL"),
        messages=message_history,
        tools=tools,
        tool_choice="auto",
        timeout=300.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
        temperature=0
    )
    #print ("full_resp:\n", full_resp)
    openai_message = full_resp.choices[0].message
    content = openai_message.content or ""

    # å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚
    if full_resp.choices[0].message.tool_calls:
        # å–ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨
        tool_call = full_resp.choices[0].message.tool_calls[0]
        tool_use_id = tool_call.id
        tool_name = tool_call.function.name
        tool_input = json.loads(tool_call.function.arguments)
        
        # è°ƒç”¨å…¶ä»–MCPå·¥å…·
        result = await call_mcp_tool(tool_name, tool_input)
        print ("results:\n",result)

        message_history = cl.user_session.get("message_history", [])
        # å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
        message_history.extend([
            {
                "role": "assistant",
                "content": None,
                "tool_calls": [
                    {
                        "id": tool_use_id,
                        "type": "function",
                        "function": {
                            "name": tool_name,
                            "arguments": json.dumps(tool_input)
                        }
                    }
                ]
            },
            {
                "role": "tool",
                "content": str(result),
                "tool_call_id": tool_use_id
            }
        ])
        return tool_use_id
    else:
        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå“åº”
        await handle_text_input(message_history , content)
        return None


# ======================== python_execå‡½æ•°çš„codeä¿®å¤ç¼©è¿›ä¸å­—ç¬¦è½¬ä¹‰ ========================
def fix_indentation(code_str):
    lines = []
    indent_level = 0
    block_keywords = {'for', 'if', 'else', 'elif', 'while', 'def', 'class', 'try', 'except', 'with'}
    
    for line in code_str.split('\n'):
        stripped = line.strip()
        if not stripped:
            lines.append('')
            continue
        
        # æ£€æµ‹æ˜¯å¦æ˜¯å—å…³é”®å­—ï¼ˆå¦‚ if, for ç­‰ï¼‰
        is_block = any(
            re.match(rf'^{kw}\b.*:$', stripped)
            for kw in block_keywords
        )
        
        # å¤„ç† else/elif çš„ç‰¹æ®Šæƒ…å†µ
        if stripped.startswith(('else', 'elif')):
            indent_level = max(0, indent_level - 1)
        
        # æ·»åŠ å½“å‰ç¼©è¿›
        lines.append('    ' * indent_level + stripped)
        
        # å¦‚æœæ˜¯å—å…³é”®å­—ï¼Œå¢åŠ ç¼©è¿›
        if is_block:
            indent_level += 1
            
    return '\n'.join(lines)


# ======================== å·¥å…·è°ƒç”¨å¤„ç† ========================
@cl.step(type="tool")
async def process_tool_calls(openai_message: ChatCompletionMessage) -> dict:
    """å¤„ç†å·¥å…·è°ƒç”¨, æ–°å¢tool_call_idç”Ÿæˆ"""
    print ("tool_calls:\n", openai_message.tool_calls)

    results = []
    for tool_call in openai_message.tool_calls:
        function_name = tool_call.function.name
        arguments_str = tool_call.function.arguments

        print ("* åŸå§‹arguments_str:\n", arguments_str)
        tool_call_id = tool_call.id 

        try:
            arguments = json.loads(arguments_str)
        except:
            arguments = ast.literal_eval(
                arguments_str
            )
        print ("* ç¬¬ä¸€æ­¥arguments:\n", arguments)

        if 'code' in arguments:
            code_str = arguments['code']

            try:
                # å¤„ç†ä»£ç å—æ ‡è®°
                if code_str.startswith('```py'):
                    code_str = code_str[5:].lstrip()
                if code_str.endswith('```'):
                    code_str = code_str[:-3].rstrip()
                # æ›´æ–°å¹¶é‡æ–°åºåˆ—åŒ–
                arguments['code'] = code_str
        
            except json.JSONDecodeError:
                # éJSONæ ¼å¼åˆ™ç›´æ¥å¤„ç†åŸå§‹å­—ç¬¦ä¸²
                if arguments.startswith('```py'):
                    arguments = arguments[5:].lstrip()
                if arguments.endswith('```'):
                    arguments = arguments[:-3].rstrip()

            print ("* ç¬¬äºŒæ­¥arguments:\n", arguments)
        

            # æ£€æµ‹try/exceptå­˜åœ¨æ—¶ä¸ä¿®å¤ç¼©è¿›
            if 'try' not in code_str and 'except' not in code_str:
                code_str = (
                    arguments['code']
                    #.replace('\\n', '\n')
                    #.replace('\\t', "'\\t'")  # å­—ç¬¦ä¸²æ›¿æ¢ï¼šç›´æ¥æ›¿æ¢ \\t ä¸º '\t'ã€‚åŸå­—ç¬¦ä¸²ä¸­çš„ \\t è¡¨ç¤ºå­—é¢çš„åæ–œæ å’Œ tï¼Œæ›¿æ¢å '\t' ä¸­çš„ \t ä¼šè¢« Python è¯†åˆ«ä¸ºè½¬ä¹‰åçš„åˆ¶è¡¨ç¬¦ã€‚
                    #.replace("\\'", "'")   
                    #.replace('\\"', '"')
                    #.replace("\\'", "'")   #  æ­¥éª¤è§£é‡Šå™¨çš„ä»£ç å¤šå‡ºç°\'  åŸå§‹ä»£ç æ˜¯'    å¯ä»¥é—®å¤§æ¨¡å‹ \'è½¬æ¢æˆ'ï¼Œä»£ç å¦‚ä½•å®ç°ï¼Œç”¨replaceæ–¹æ³•
                )
                # ä¿®å¤ç¼©è¿›
                code_str = fix_indentation(code_str)
                arguments['code'] = code_str
                print ("* ç¬¬ä¸‰æ­¥arguments:\n", arguments)

        current_step = cl.context.current_step
        current_step.name = function_name
        if current_step.name == "python_exec":
            # è¾“å‡ºpythonè¯­è¨€çš„è¾“å…¥æ­¥éª¤è§£é‡Šå™¨
            arguments_code_py = f"```py\n{arguments['code']}\n```"
            current_step.input = arguments_code_py
        else:
            current_step.input = arguments

        try:
            if function_name == "python_exec":
                function_response = await function_manager.call_function(function_name, arguments)

            else:
                function_response = await function_manager.call_function(function_name, arguments)
            current_step.output = function_response
            current_step.language = "json"
            results.append({
                "tool_call_id": tool_call_id,
                "function_name": function_name,
                "arguments": arguments,
                "function_response": function_response
            })

        except Exception as e:
            print(f"å‡½æ•°{function_name}æ‰§è¡Œå¤±è´¥: {str(e)}")
            results.append({
                "tool_call_id": tool_call_id,
                "function_name": function_name,
                "arguments": arguments,
                "function_response": f"å‡½æ•°{function_name}æ‰§è¡Œå¤±è´¥: {str(e)}"
            })
    
    return results


# ======================== function_callsæµå¼è¯·æ±‚å¤„ç† ========================
async def tool_calls(message_history: list):
    # ç¬¬ä¸€é˜¶æ®µï¼šå·¥å…·è°ƒç”¨å¤„ç†ï¼ˆéæµå¼ï¼‰
    full_resp = await client.chat.completions.create(
        model=os.environ.get("LLM_MODEL"),
        messages=message_history,
        tools=tools,
        tool_choice="auto",
        timeout=300.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
        temperature=0
    )
    #print ("full_resp:\n", full_resp)
    openai_message = full_resp.choices[0].message
    content = openai_message.content or ""

    
    #å¦‚æœå˜é‡tool_callså±æ€§ä¸ºç©ºï¼Œå°±è°ƒç”¨`handle_text_input`å‡½æ•°, å¹¶ä¸”è¿”å›`None`ä»£è¡¨æ²¡æœ‰å‡½æ•°å¯ä»¥è°ƒç”¨ï¼Œè·³å‡ºå¾ªç¯ã€‚
    if not openai_message.tool_calls:
        await handle_text_input(message_history , content)
        return None
    else:
        # å¤„ç†å·¥å…·è°ƒç”¨
        results = await process_tool_calls(openai_message)
        #print ("results:\n",results)
        if results:  # results æ˜¯ä¸€ä¸ªåˆ—è¡¨
            for result in results:  # éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªç»“æœ
            
                # æ˜¾ç¤ºå‡½æ•°è¿”å›ç»“æœ
                if result['function_name'] == "python_exec":
                    if "function_response" in result and "success" in result['function_response'].lower():
                        message_content = f"python_exec å‡½æ•°æ‰§è¡ŒæˆåŠŸï¼Œè¿”å›ç»“æœå¦‚ä¸‹ï¼š\n{result['function_response']}"
                    else:
                        message_content = f"python_exec å‡½æ•°æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç»“æœå¦‚ä¸‹ï¼š\n{result.get('function_response', 'æ— è¿”å›ç»“æœ')}"
                else:
                    message_content = f"{result['function_name']} å‡½æ•°æ‰§è¡ŒæˆåŠŸï¼Œè¿”å›ç»“æœå¦‚ä¸‹ï¼š\n{result['function_response']}"


            message_history = cl.user_session.get("message_history", [])
            print ("message_content:\n",message_content)
            message_history.append({"role": "user", "content": message_content})

        return results

# ======================== å›¾ç‰‡OCRå¤„ç†æ¨¡å— ========================
# async def handle_image_ocr(message: cl.Message):
#     if not message.elements or not message.elements[0].mime.startswith("image"):
#         await cl.Message("âŒ è¯·ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶").send()
#         return
    
#     image = message.elements[0]
#     await cl.Message("ğŸ–¼ï¸ æ­£åœ¨è¿›è¡ŒOCRå¤„ç†...").send()

#     await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°

#     try:
#         ocr_text = await process_image_ocr(image.path)
#         if ocr_text:
#             await cl.Message(f"**OCRè¯†åˆ«å†…å®¹**\n\n{ocr_text}").send()
#         else:
#             await cl.Message("âŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬").send()
#     except Exception as e:
#         await cl.Message(f"âŒ OCRå¤„ç†å¤±è´¥: {str(e)}").send()
#     finally:
#         message.elements = []  # æ¸…ç©ºæ¶ˆæ¯ä¸­çš„å›¾ç‰‡å…ƒç´ 

# ======================== è¯­éŸ³å›å¤å¤„ç†æ¨¡å— ========================
async def handle_voice_mode(message: cl.Message):
    global tts

    voice_mode_response = cl.user_session.get("voice_mode_response")

    if not voice_mode_response:
        response = message.content
    else:
        response = await handle_text_input(
            cl.user_session.get("conversation_history", []),
            message.content
        )
    msg = cl.Message(content=f"è¯­éŸ³ç”Ÿæˆä¸­...")
    await msg.send()

    print("å½“å‰ç›®å½•:", os.getcwd())  #å½“å‰ç›®å½•: D:\chainlit\chainlit-datalayer\demo_app\tmp
    # ç”Ÿæˆè¯­éŸ³
    audio_file = os.path.join("..", str(TEMP_DIR), f"{uuid.uuid4()}.wav")
    print("audio_file:", audio_file)

    #ref_text = tts.transcribe("../tts_temp/seedtts_ref_zh_1.wav") 

    async def tts_infer(response , audio_file):
        tts.infer(
                ref_file="../tts_temp/seedtts_ref_zh_1.wav",  #../æ˜¯ä¸Šä¸€çº§ç›®å½•
                ref_text="å¯¹äºç–«æƒ…å¤§å®¶ä¸è¦è½»è§†ä½†ä¹Ÿä¸ç”¨è¿‡åº¦ææ…Œåšåˆ°ä¸å“„æŠ¢ç‰©å“ä¸å“„æŠ¬ç‰©ä»·.",  # ä¼ é€’ç©ºå­—ç¬¦ä¸²å ä½
                gen_text=response,
                file_wave=str(audio_file),
            )
        return audio_file
    
    await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°

    # ä½¿ç”¨å¼‚æ­¥çº¿ç¨‹æ‰§è¡Œé˜»å¡æ“ä½œ
    loop = asyncio.get_event_loop()
    audio_file = await loop.run_in_executor(None, lambda: asyncio.run(tts_infer(response , str(audio_file))))

    # è¯»å–å¹¶å‘é€éŸ³é¢‘
    #with open(audio_file, "rb") as f:
    #    audio_data = f.read()

    await msg.remove()
    
    audio_element = cl.Audio(
        path=str(audio_file),  # æ˜¾å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        #content=audio_data,
        mime="audio/wav",
        auto_play=True
    )

    #audio_file.resolve().unlink(missing_ok=True)  # æ›´å®‰å…¨çš„æ¸…ç†

    await asyncio.sleep(0.1)  # æ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿UIæ›´æ–°
    
    await cl.Message(
        content="",
        elements=[audio_element]
    ).send()

# ======================== ai_å›¾ç‰‡OCRå¤„ç†æ¨¡å— ========================
# å®šä¹‰å‡½æ•°ï¼šæ ¹æ®æ—¶é—´èŒƒå›´å’Œè§†é¢‘å‚æ•°è®¡ç®—éœ€è¦æŠ½å–çš„å¸§ç´¢å¼•
def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):
    """å‚æ•°è¯´æ˜ï¼š
    bound: æ—¶é—´èŒƒå›´å…ƒç»„(start_sec, end_sec)
    fps: è§†é¢‘å¸§ç‡ï¼ˆå¸§/ç§’ï¼‰
    max_frame: è§†é¢‘æ€»å¸§æ•°
    first_idx: èµ·å§‹å¸§ç´¢å¼•ï¼ˆé»˜è®¤0ï¼‰
    num_segments: éœ€è¦åˆ†å‰²çš„è§†é¢‘æ®µæ•°ï¼ˆé»˜è®¤32ï¼‰"""
    
    # å¤„ç†æ—¶é—´è¾¹ç•Œ
    if bound:  # å¦‚æœæŒ‡å®šäº†æ—¶é—´èŒƒå›´
        start, end = bound[0], bound[1]  # è·å–å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    else:  # æœªæŒ‡å®šåˆ™ä½¿ç”¨æå¤§èŒƒå›´
        start, end = -100000, 100000
    
    # è®¡ç®—èµ·å§‹å’Œç»“æŸå¸§ç´¢å¼•
    start_idx = max(first_idx, round(start * fps))  # è½¬æ¢ä¸ºå¸§ç´¢å¼•ï¼Œç¡®ä¿ä¸å°äºfirst_idx
    end_idx = min(round(end * fps), max_frame)  # ç»“æŸå¸§ä¸è¶…è¿‡è§†é¢‘æœ€å¤§å¸§
    
    # è®¡ç®—æ¯ä¸ªè§†é¢‘æ®µçš„é•¿åº¦ï¼ˆä»¥å¸§ä¸ºå•ä½ï¼‰
    seg_size = float(end_idx - start_idx) / num_segments
    
    # ç”Ÿæˆæ¯ä¸ªè§†é¢‘æ®µçš„ä¸­å¿ƒå¸§ç´¢å¼•
    frame_indices = np.array([
        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))
        for idx in range(num_segments)
    ])
    return frame_indices  # è¿”å›32ä¸ªå‡åŒ€åˆ†å¸ƒçš„å¸§ç´¢å¼•


# å®šä¹‰è§†é¢‘åŠ è½½å’Œå¤„ç†å‡½æ•°
def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):
    """å‚æ•°è¯´æ˜ï¼š
    video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
    bound: æ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰
    input_size: è¾“å…¥å›¾åƒå°ºå¯¸ï¼ˆé»˜è®¤448x448ï¼‰
    max_num: æœ€å¤§åˆ†å—æ•°é‡ï¼ˆåŠ¨æ€é¢„å¤„ç†ç”¨ï¼‰
    num_segments: è§†é¢‘åˆ†å‰²æ®µæ•°"""
    
    # åˆå§‹åŒ–è§†é¢‘é˜…è¯»å™¨
    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)  # ä½¿ç”¨CPUå•çº¿ç¨‹è¯»å–
    max_frame = len(vr) - 1  # è·å–è§†é¢‘æ€»å¸§æ•°ï¼ˆç´¢å¼•ä»0å¼€å§‹ï¼‰
    fps = float(vr.get_avg_fps())  # è·å–è§†é¢‘å¹³å‡å¸§ç‡

    # åˆå§‹åŒ–å­˜å‚¨å®¹å™¨
    pixel_values_list = []  # å­˜å‚¨å¤„ç†åçš„å›¾åƒå¼ é‡
    num_patches_list = []   # å­˜å‚¨æ¯å¸§çš„åˆ†å—æ•°é‡
    
    # åˆ›å»ºå›¾åƒé¢„å¤„ç†æµæ°´çº¿
    transform = build_transform(input_size=input_size)  # åŒ…å«ç¼©æ”¾ã€å½’ä¸€åŒ–ç­‰æ“ä½œ
    
    # è·å–éœ€è¦å¤„ç†çš„å¸§ç´¢å¼•
    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)
    
    # éå†æ¯ä¸ªé€‰å®šå¸§è¿›è¡Œå¤„ç†
    for frame_index in frame_indices:
        # è¯»å–å¸§å¹¶è½¬æ¢ä¸ºPILå›¾åƒ
        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')
        
        # åŠ¨æ€é¢„å¤„ç†ï¼ˆå¯èƒ½åŒ…å«åˆ†å—ã€ç¼©ç•¥å›¾å¤„ç†ç­‰ï¼‰
        img = dynamic_preprocess(img, 
                                image_size=input_size,
                                use_thumbnail=True,
                                max_num=max_num)
        
        # å¯¹æ¯ä¸ªåˆ†å—åº”ç”¨é¢„å¤„ç†
        pixel_values = [transform(tile) for tile in img]
        pixel_values = torch.stack(pixel_values)  # å †å åˆ†å—å¼ é‡
        
        # è®°å½•åˆ†å—æ•°é‡å’Œé¢„å¤„ç†ç»“æœ
        num_patches_list.append(pixel_values.shape[0])
        pixel_values_list.append(pixel_values)
    
    # åˆå¹¶æ‰€æœ‰å¸§çš„åˆ†å—æ•°æ®
    pixel_values = torch.cat(pixel_values_list)
    return pixel_values, num_patches_list



# ImageNetæ•°æ®é›†çš„æ ‡å‡†åŒ–å‚æ•°
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)
# ai_ocrå›¾åƒé¢„å¤„ç†
def build_transform(input_size):
    """æ„å»ºå›¾åƒé¢„å¤„ç†æµæ°´çº¿"""
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    return T.Compose([
        # ç¡®ä¿å›¾åƒä¸ºRGBæ ¼å¼
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        # è°ƒæ•´å¤§å°å¹¶ä½¿ç”¨åŒä¸‰æ¬¡æ’å€¼
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),  # è½¬æ¢ä¸ºå¼ é‡
        T.Normalize(mean=MEAN, std=STD)  # æ ‡å‡†åŒ–
    ])

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    """å¯»æ‰¾æœ€æ¥è¿‘çš„ç›®æ ‡å®½é«˜æ¯”"""
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height  # åŸå§‹å›¾åƒé¢ç§¯
    # éå†æ‰€æœ‰å€™é€‰æ¯”ä¾‹
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        # é€‰æ‹©å·®å¼‚æœ€å°çš„æ¯”ä¾‹
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:  # ç›¸åŒå·®å¼‚æ—¶é€‰æ‹©é¢ç§¯æ›´å¤§çš„
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    """åŠ¨æ€å›¾åƒé¢„å¤„ç†ï¼šå°†å›¾åƒåˆ†å‰²ä¸ºå¤šä¸ªå­å›¾"""
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height  # åŸå§‹å®½é«˜æ¯”
    
    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å®½é«˜æ¯”ç»„åˆ
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) 
        for i in range(1, n + 1) for j in range(1, n + 1) 
        if i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    
    # æ‰¾åˆ°æœ€ä½³æ¯”ä¾‹å¹¶è®¡ç®—ç›®æ ‡å°ºå¯¸
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]  # æ€»å—æ•°
    
    # è°ƒæ•´å¤§å°å¹¶åˆ†å‰²å›¾åƒ
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        # è®¡ç®—æ¯ä¸ªå­å›¾çš„åæ ‡
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        processed_images.append(resized_img.crop(box))  # è£å‰ªå­å›¾
        
    # å¯é€‰æ·»åŠ ç¼©ç•¥å›¾
    if use_thumbnail and len(processed_images) != 1:
        processed_images.append(image.resize((image_size, image_size)))
    return processed_images

def ai_ocr_preprocess(image_file, input_size=448, max_num=12):
        """åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ"""
        image = Image.open(image_file).convert('RGB')
        transform = build_transform(input_size)
        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
        pixel_values = [transform(img) for img in images]  # åº”ç”¨é¢„å¤„ç†
        return torch.stack(pixel_values)  # å †å ä¸ºå¼ é‡


async def handle_image_ai_ocr(message: cl.Message):
    global ocr_model
    global ocr_tokenizer
    global num_patches_list
    global video_prefix
    #ocr_model = cl.user_session.get("ocr_model")
    #ocr_tokenizer = cl.user_session.get("ocr_tokenizer")
    conversation_history = cl.user_session.get("conversation_history", [])

    if message.elements:  # å…ˆåˆ¤æ–­elementsæ˜¯å¦å­˜åœ¨ä¸”éç©º
        cl.user_session.set("last_one_msg_elements", message.elements[-1])

    last_one_msg_elements = cl.user_session.get("last_one_msg_elements", None)
    
    # éªŒè¯å›¾ç‰‡ä¸Šä¼ 
    if not last_one_msg_elements:
        pixel_values = cl.user_session.get("pixel_values", None)

    if last_one_msg_elements:
        if last_one_msg_elements.mime.startswith("video"):  
            video_element = last_one_msg_elements
            print(f"Received video: {video_element.path}")
            video_path = video_element.path
            # åŠ è½½å¹¶å¤„ç†è§†é¢‘ï¼ˆè®¾ç½®8ä¸ªæ—¶é—´æ®µï¼Œæ¯ä¸ªæ—¶é—´å–1ä¸ªåˆ†å—ï¼‰
            pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)

            # ç¡®ä¿æ•°æ®åœ¨GPUä¸Š
            if torch.cuda.is_available():
                # å°†æ•°æ®è½¬æ¢ä¸ºbfloat16æ ¼å¼å¹¶è½¬ç§»åˆ°GPU
                pixel_values = pixel_values.to(torch.bfloat16).cuda()
                
            # æ„é€ è§†é¢‘å‰ç¼€ï¼šä¸ºæ¯ä¸ªå¸§ç”Ÿæˆ"FrameX: <image>\n"æ ¼å¼çš„æ–‡æœ¬
            video_prefix = ''.join([f'Frame{i+1}: <image>\n' for i in range(len(num_patches_list))])
            cl.user_session.set("pixel_values", pixel_values)

    if last_one_msg_elements:
        if last_one_msg_elements.mime.startswith("image"): 
            # è·å–å›¾ç‰‡è·¯å¾„å¹¶é¢„å¤„ç†
            image_element = last_one_msg_elements
            print(f"Received image: {image_element.path}")
            # è½¬æ¢å¼ é‡åˆ°æ¨¡å‹è®¾å¤‡
            pixel_values = ai_ocr_preprocess(image_element.path).to(torch.bfloat16)
            # ç¡®ä¿æ•°æ®åœ¨GPUä¸Š
            if torch.cuda.is_available():
                pixel_values = pixel_values.to('cuda')
            num_patches_list = [pixel_values.size(0)]
            cl.user_session.set("pixel_values", pixel_values)

    msg = await cl.Message("ğŸ–¼ï¸ æ­£åœ¨è¿›è¡ŒOCRå¤„ç†...").send()
    await asyncio.sleep(0.1)  # ç¡®ä¿UIæ›´æ–°
    
    try:
        # å‡†å¤‡æµå¼è¾“å‡º
        streamer = TextIteratorStreamer(ocr_tokenizer, timeout=60)
        generation_config = {
            "max_new_tokens": 4000,
            "do_sample": False,
            "streamer": streamer
        }
        
        # åˆ›å»ºæ¶ˆæ¯å¯¹è±¡
        msg_ocr = cl.Message(content="")
        await msg_ocr.send()


        if last_one_msg_elements:
            if last_one_msg_elements.mime.startswith("image"):  # æ–°å¢è§†é¢‘å¤„ç†åˆ†æ”¯
                image_message_describe = cl.user_session.get("image_describe")
                print ("image_describe:\n",image_message_describe)
                if image_message_describe and len(message.content) < 2:
                    message.content = image_message_describe
            elif last_one_msg_elements.mime.startswith("video"):  # æ–°å¢è§†é¢‘å¤„ç†åˆ†æ”¯
                video_message_describe = "è¯¦ç»†æè¿°è¿™ä¸ªè§†é¢‘"
                print ("video_describe:\n",video_message_describe)
                if video_message_describe and len(message.content) < 2:
                    message.content = video_message_describe
        else:
            message.content = message.content


        if last_one_msg_elements:
            print ("last_one_msg_elements:\n",last_one_msg_elements)
            if last_one_msg_elements.mime.startswith("video"):  # æ–°å¢è§†é¢‘å¤„ç†åˆ†æ”¯
                # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
                Thread(target = ocr_model.chat, kwargs={
                    "tokenizer": ocr_tokenizer,
                    "pixel_values": pixel_values,  # ä½¿ç”¨å¤„ç†åçš„å¼ é‡
                    "question": (video_prefix + message.content),
                    "generation_config": generation_config,
                    "num_patches_list": num_patches_list,
                    "history": conversation_history,  # å…³é”®ä¿®æ”¹ï¼šæ¯æ¬¡ä½¿ç”¨ç©ºå†å²
                    "return_history": False  # ä¸å†éœ€è¦è¿”å›å†å²
                }).start()

            if last_one_msg_elements.mime.startswith("image"):  # æ–°å¢è§†é¢‘å¤„ç†åˆ†æ”¯
                # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
                Thread(target = ocr_model.chat, kwargs={
                    "tokenizer": ocr_tokenizer,
                    "pixel_values": pixel_values,  # ä½¿ç”¨å¤„ç†åçš„å¼ é‡
                    "question": ("<image>\n" + message.content),
                    "generation_config": generation_config,
                    "num_patches_list": num_patches_list,
                    "history": conversation_history,  # å…³é”®ä¿®æ”¹ï¼šæ¯æ¬¡ä½¿ç”¨ç©ºå†å²
                    "return_history": False  # ä¸å†éœ€è¦è¿”å›å†å²
                }).start()
        else:
            # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
            print ("message.content:\n",message.content)
            Thread(target = ocr_model.chat, kwargs={
                "tokenizer": ocr_tokenizer,
                "pixel_values": None,  # ä½¿ç”¨å¤„ç†åçš„å¼ é‡
                "question": message.content,
                "generation_config": generation_config,
                "history": conversation_history,  # å…³é”®ä¿®æ”¹ï¼šæ¯æ¬¡ä½¿ç”¨ç©ºå†å²
                "return_history": False  # ä¸å†éœ€è¦è¿”å›å†å²
            }).start()   

        # æµå¼å“åº”
        response = ''
        # Loop through the streamer to get the new text as it is generated
        for token in streamer:
            if token == ocr_model.conv_template.sep:
                continue
            #print(token, end="\n", flush=True)  # Print each new chunk of generated text on the same line
            if "<|im_end|>" in token:
                token = token.replace("<|im_end|>", "")
                if token:  # å¦‚æœåˆ é™¤åè¿˜æœ‰å†…å®¹ï¼Œç»§ç»­å‘é€å‰©ä½™éƒ¨åˆ†
                    await msg_ocr.stream_token(token)
                continue
            #response += token
            await msg_ocr.stream_token(token)
        
        response = msg_ocr.content

        if response.startswith('\[') and response.endswith('\]'):
            response = f"$${response[2:-2]}$$"

        if response == "content" or response == "role":
            response = "å›¾ç‰‡æ²¡æœ‰æ–‡å­—"
        print(f"æœ€ç»ˆresponse:\n{response}")
        msg_ocr.content = response
        await msg_ocr.update()
        # æ›´æ–°æ¶ˆæ¯å‰æ‰‹åŠ¨è®¾ç½®æœ€ç»ˆå†…å®¹
        await msg.remove()

        conversation_history.extend([
            {"role": "user", "content":message.content},
            {"role": "assistant", "content": response}
        ])
        
        cl.user_session.set("conversation_history", conversation_history)

        
    except Exception as e:
        await cl.Message(f"âŒ å¤„ç†å¤±è´¥: {str(e)}").send()


# ======================== å›¾ç‰‡å¤„ç†æ¨¡å— ========================
async def handle_image_input(message: cl.Message, history: list):
    image = message.elements[0]
    print(f"Received image: {image.path}")

    query_msg = None  # æ˜¾å¼åˆå§‹åŒ–
    try:
        query_msg = await cl.Message("ğŸ–¼ï¸ æ­£åœ¨åˆ†æå›¾ç‰‡...").send()
        # åŸæœ‰å›¾ç‰‡å¤„ç†é€»è¾‘
        
        response = await process_image(
            image.path,
            message.content if (message.content and len(message.content) > 2) 
            else "Describe this image."
        )
        full_response = f"**å›¾ç‰‡è¯†åˆ«è‹±æ–‡å†…å®¹**\n\n{response}"
        await cl.Message(full_response).send()
        history.extend([
            {"role": "user", "content": message.content if (message.content and len(message.content) > 2) 
            else "Describe this image."},
            {"role": "assistant", "content": full_response}
        ])
    except Exception as e:
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        if 'query_msg' in locals():
            query_msg.content = error_msg
            await query_msg.update()
        else:
            await cl.Message(content=error_msg).send()
        
    cl.user_session.set("conversation_history", history)


# ======================== å›¾ç‰‡å¤„ç†æ¨¡å—-è¾…åŠ©å‡½æ•° ========================
async def process_image(image_path, prompt):
    # å°†åŒæ­¥é˜»å¡æ“ä½œåŒ…è£…åˆ°å¼‚æ­¥çº¿ç¨‹ä¸­
    def _sync_process():
        image = Image.open(image_path)
        print("Image opened successfully")
        inputs = image_processor.process(images=[image], text=prompt)
        print("Image processed successfully")
        inputs = {k: v.to(image_model.device).unsqueeze(0) for k, v in inputs.items()}
        print("Inputs prepared successfully")
        output = image_model.generate_from_batch(
            inputs,
            GenerationConfig(
                max_new_tokens=400,
                stop_strings="<|endoftext|>",
                do_sample=True,  # å¯ç”¨é‡‡æ ·
                temperature=0.6,
                top_k=40,
                top_p=0.9,
            ),
            tokenizer=image_processor.tokenizer
        )
        decoded_output = image_processor.tokenizer.decode(output[0, inputs['input_ids'].size(1):], skip_special_tokens=True)
        print(f"Decoded output: {decoded_output}")
        return decoded_output
    try:
        # ä½¿ç”¨å¼‚æ­¥çº¿ç¨‹æ‰§è¡Œé˜»å¡æ“ä½œ
        loop = asyncio.get_event_loop()
        decoded_output = await loop.run_in_executor(None, _sync_process)
        return decoded_output
    except Exception as e:
        return f"å›¾ç‰‡å¤„ç†é”™è¯¯: {str(e)}"

            
# ======================== çŸ¥è¯†æŸ¥è¯¢æ¨¡å— ========================
async def handle_post_upload_actions(user_input: str):
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    query_msg = None  # æ˜¾å¼åˆå§‹åŒ–
    query = user_input

    api_key = os.getenv("LIGHTRAG_API_KEY")
    if not api_key:
        raise ValueError("LIGHTRAG_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼")
    client = LightRAGClient(api_key=api_key)

    # è‡ªå®šä¹‰é…ç½®
    # mode="local", "global", "hybrid", "naive", "mix", "bypass"
    # response_type='Multiple Paragraphs', 'Single Paragraph', 'Bullet Points'
    custom_config = QueryConfig(
        mode="mix",
        response_type="Bullet Points",
    )

    if not (query := query.strip()):
        await cl.Message("âŒ æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º").send()
        return 
    query_msg = await cl.Message("").send()

    #è¿™æ˜¯ä½¿ç”¨miniragï¼Œ mode: æŒ‡ä»¤æœ‰è¿™äº›["light", "naive", "mini"] = "mini"
    #response = rag.query(query, param=QueryParam(mode="light"))
    #await cl.Message(f"**çŸ¥è¯†åº“å›ç­”**\n\n{response}").send()

    # æ‰§è¡ŒæŸ¥è¯¢
    full_response = ""
    chunk = ""
    try:
        # ä½¿ç”¨å¼‚æ­¥çº¿ç¨‹æ‰§è¡Œé˜»å¡æ“ä½œ
        loop = asyncio.get_event_loop()
        for chunk in await loop.run_in_executor(
            None, 
            lambda: list(client.query(question=query, config=custom_config))
        ):
            full_response += chunk
            await query_msg.stream_token(chunk)
            await query_msg.update()
            #print(chunk, end="\n", flush=True)

        query_msg.content = full_response
        await query_msg.update()
    
    # ä¿®æ”¹å¼‚å¸¸å¤„ç†éƒ¨åˆ†ï¼Œæ·»åŠ å“åº”å†…å®¹æ‰“å°
    except requests.exceptions.RequestException as e:
        error_msg = "âš ï¸ çŸ¥è¯†åº“æœåŠ¡è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥:\n"
        # å¤„ç†æ— å“åº”å¯¹è±¡çš„æƒ…å†µï¼ˆå¦‚è¿æ¥æ‹’ç»ï¼‰
        if not hasattr(e, "response") or e.response is None:
            error_msg += f"- æœåŠ¡æœªè¿è¡Œæˆ–é…ç½®é”™è¯¯\n- é”™è¯¯è¯¦æƒ…: {str(e)}"
        else:
            try:
                error_detail = e.response.json().get("detail", str(e))
            except (json.JSONDecodeError, AttributeError):
                error_detail = e.response.text[:500]  # æˆªæ–­é¿å…è¿‡é•¿
            
            error_msg += (
                f"- çŠ¶æ€ç : {getattr(e.response, 'status_code', 'æœªçŸ¥')}\n"
                f"- é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"- è¯¦æƒ…: {error_detail}"
            )

        # è¡¥å……è¿æ¥é—®é¢˜æ’æŸ¥å»ºè®®
        error_msg += "\n\næ’æŸ¥æ­¥éª¤:\n1. ç¡®è®¤LightRAGæœåŠ¡å·²å¯åŠ¨\n2. æ£€æŸ¥ç«¯å£9721æ˜¯å¦ç›‘å¬\n3. éªŒè¯APIå¯†é’¥é…ç½®"

        if 'query_msg' in locals():
            query_msg.content = error_msg
            await query_msg.update()
        else:
            await cl.Message(content=error_msg).send()


# ======================== è”ç½‘æœç´¢æ¨¡å— ========================
async def handle_web_search(user_input: str):
    web_search_msg = None  # æ˜¾å¼åˆå§‹åŒ–
    try:
        query = user_input
        if not (query := query.strip()):
            await cl.Message("âŒ æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º").send()
            return 
        web_search_msg = await cl.Message(f"ğŸ” æ­£åœ¨è”ç½‘æœç´¢: {query}").send()
        search_result = asyncio.run(search_web(query))
        print ("search_web:\n" , search_result)
        await cl.Message(f"**è”ç½‘æœç´¢ç»“æœ**\n\n{search_result}").send()

    except Exception as e:
        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        error_msg = f"âŒ è”ç½‘æœç´¢å¤±è´¥: {str(e)}"
        if 'web_search_msg' in locals():
            web_search_msg.content = error_msg
            await web_search_msg.update()
        else:
            await cl.Message(content=error_msg).send()


# ======================== è”ç½‘æœç´¢æ¨¡å—-è¾…åŠ©å‡½æ•° ========================
async def search_web(query: str) -> str:
    url = f'https://duckduckgo.com/html/?q={urllib.parse.quote(query)}'
    
    # éšæœºç”¨æˆ·ä»£ç†åˆ—è¡¨
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15',
        'Mozilla/5.0 (Linux; Android 10; SM-G981B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Mobile Safari/537.36'
    ]

    try:
        # éšæœºé€‰æ‹©ç”¨æˆ·ä»£ç†å¹¶è®¾ç½®è¶…æ—¶
        response = requests.get(
            url,
            headers={'User-Agent': random.choice(USER_AGENTS)},
            timeout=10  # 10ç§’è¶…æ—¶
        )
        response.raise_for_status()
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        results = []
        seen_urls = set()  # ç”¨äºURLå»é‡
        
        for result in soup.select('.result__a'):
            title = result.get_text().strip()
            href = result['href']
            
            try:
                # è§£æçœŸå®URL
                if href.startswith('/l/?uddg='):
                    parsed = urllib.parse.urlparse(href)
                    params = urllib.parse.parse_qs(parsed.query)
                    encoded_url = params.get('uddg', [href])[0]
                    decoded_url = urllib.parse.unquote(encoded_url)
                    
                    # äºŒæ¬¡è§£ææ¸…ç†è·Ÿè¸ªå‚æ•°
                    final_url = urllib.parse.urlparse(decoded_url)
                    clean_query = urllib.parse.parse_qs(final_url.query)
                    # ç§»é™¤å¸¸è§è·Ÿè¸ªå‚æ•°
                    for param in ['utm_', 'fbclid', 'gclid']:
                        clean_query = {k: v for k, v in clean_query.items() if not k.startswith(param)}
                    
                    # é‡å»ºå¹²å‡€URL
                    final_url = final_url._replace(
                        query=urllib.parse.urlencode(clean_query, doseq=True),
                        fragment=''  # ç§»é™¤é”šç‚¹
                    ).geturl()
                else:
                    final_url = urllib.parse.urlparse(href)._replace(query='', fragment='').geturl()

                # æ ‡å‡†åŒ–URLå¹¶å»é‡
                final_url = final_url.split('#')[0].rstrip('/')  # ç»Ÿä¸€æ ¼å¼
                if final_url.lower() in seen_urls:
                    continue
                seen_urls.add(final_url.lower())

                # éªŒè¯æœ‰æ•ˆURLæ ¼å¼
                if re.match(r'^https?://(?:www\.)?[a-zA-Z0-9-]+\.[a-zA-Z]{2,}', final_url):
                    results.append((title, final_url))
                
            except Exception as e:
                print(f"è§£æURLæ—¶å‡ºé”™: {str(e)}")
                continue

        # éšæœºæ’åºå¹¶é™åˆ¶ç»“æœæ•°é‡
        random.shuffle(results)
        MAX_RESULTS = 10
        formatted_results = [f"ğŸ”— **{title}**\n{url}" for title, url in results[:MAX_RESULTS]]
        
        return "\n\n".join(formatted_results) if formatted_results else "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"

    except requests.exceptions.Timeout:
        return "â³ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•"
    except requests.RequestException as e:
        return f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
    except Exception as e:
        return f"âš ï¸ å¤„ç†å‡ºé”™: {str(e)}"
    
# ======================== OpenAIæ–‡æœ¬å¤„ç†æ¨¡å— ========================
async def handle_text_input(history: list, user_input: str):
    try:   
        # æ·»åŠ ç”¨æˆ·æ–°æ¶ˆæ¯åˆ°å†å²
        if "$$" in user_input:
            formula_analysis = """
            - **æ ¼å¼è§„èŒƒä¸ä»»åŠ¡è¦æ±‚ï¼š**
            1. ç¦æ­¢ä½¿ç”¨ \(...\) æˆ– \[...\]ã€‚
            2.æ›¿æ¢æ‰€æœ‰è¡Œé—´å…¬å¼è¯­æ³• \[...\] ä¸º $$...$$ã€‚
            3.æ›¿æ¢æ‰€æœ‰åˆ¤åˆ«å¼è¯­æ³• \(...\) ä¸º $$...$$ã€‚
            4.æ›¿æ¢æ‰€æœ‰ç³»æ•°è¯­æ³• \(...\) ä¸º $$...$$ã€‚
            5.é”™è¯¯ç¤ºä¾‹è¾“å‡ºï¼š\(a^{n+m}\)ï¼Œä¿®æ­£ï¼š$$a^{n+m}$$ï¼Œåº”ä½¿ç”¨$$åŒ…è£¹ã€‚
            ## å…¬å¼åˆ†æ
            - **ç±»å‹**: åˆ†æå…¬å¼ç±»å‹ç­‰
            - **æ­¥éª¤**: æ•°å­¦æ¨å¯¼ç­‰
            - **åº”ç”¨åœºæ™¯**: å­¦æœ¯è®ºæ–‡ã€ç‰©ç†å»ºæ¨¡ç­‰ã€‚
            """
            history.append({"role": "user", "content": "è¯·ä½¿ç”¨ä¸­æ–‡äº¤æµï¼Œ" + user_input + formula_analysis})
        else:
            history.append({"role": "user", "content": "è¯·ä½¿ç”¨ä¸­æ–‡äº¤æµï¼Œ" + user_input})
        
        msg = cl.Message(content="")
        await msg.send()

        response_stream = await client.chat.completions.create(
            model= os.environ.get("LLM_MODEL"),
            messages=history,
            max_tokens=8192,
            temperature=0.7,
            stream=True,
            timeout=300.0  # å¢åŠ è¶…æ—¶æ—¶é—´
        )
        
        full_response = ""
        token = ""
        async for chunk in response_stream:
            token = chunk.choices[0].delta.content
            if token:
                #print(token, end="\n", flush=True)
                # ç›´æ¥æ›¿æ¢æ‰€æœ‰ç›®æ ‡å­—ç¬¦ï¼Œæ— éœ€æ¡ä»¶åˆ¤æ–­
                token = token.replace("\\(", "$")  # æ›¿æ¢åæ–œæ åŠ (
                token = token.replace("\\)", "$")   # æ›¿æ¢åæ–œæ åŠ )
                token = token.replace("\\[", "$$")   # æ›¿æ¢åæ–œæ åŠ [
                token = token.replace("\\]", "$$")   # æ›¿æ¢åæ–œæ åŠ ]
                full_response += token
                await msg.stream_token(token)

        print(f"æœ€ç»ˆfull_response:\n{full_response}")
        full_response = full_response.replace("\\(", "$")  # æ›¿æ¢åæ–œæ åŠ (
        full_response = full_response.replace("\\)", "$")   # æ›¿æ¢åæ–œæ åŠ )
        full_response = full_response.replace("\\[", "$$")   # æ›¿æ¢åæ–œæ åŠ [
        full_response = full_response.replace("\\]", "$$")   # æ›¿æ¢åæ–œæ åŠ ]

        msg.content = full_response
        await msg.update()
        
        # æ·»åŠ AIå›å¤åˆ°å†å²è®°å½•
        history.append({"role": "assistant", "content": full_response})
        
        # æ›´æ–°ç”¨æˆ·ä¼šè¯ä¸­çš„å†å²è®°å½•ï¼ˆå…³é”®æ­¥éª¤ï¼‰
        cl.user_session.set("conversation_history", history)

        # è¿”å›å®Œæ•´çš„å“åº”å†…å®¹
        return full_response
        
    except Exception as e:
        error_msg = f"å¤±è´¥: {str(e)}"
        await cl.Message(content=error_msg).send()
        return None  # æˆ–è€…æ ¹æ®éœ€æ±‚æŠ›å‡ºå¼‚å¸¸
            

# ======================== qaå¤„ç†æ¨¡å— ========================
async def qa_text_input(message: cl.Message):
    try: 
        # ä»ç”¨æˆ·ä¼šè¯ä¸­è·å–å¯¹è¯é“¾
        chain = cl.user_session.get("chain")  # ConversationalRetrievalChain
        cb = cl.AsyncLangchainCallbackHandler()

        # è°ƒç”¨å¯¹è¯é“¾å¤„ç†ç”¨æˆ·æ¶ˆæ¯
        res = await chain.ainvoke({"question": message}, callbacks=[cb])
        answer = res["answer"]  # è·å–å›ç­”
        source_documents = res["source_documents"]  # è·å–æºæ–‡æ¡£

        text_elements = []  # ç”¨äºå­˜å‚¨æºæ–‡æ¡£çš„æ–‡æœ¬å…ƒç´ 

        # å¦‚æœæœ‰æºæ–‡æ¡£ï¼Œå°†å…¶æ·»åŠ åˆ°æ–‡æœ¬å…ƒç´ ä¸­
        if source_documents:
            for source_idx, source_doc in enumerate(source_documents):
                source_name = f"source_{source_idx}"
                # åˆ›å»ºæ–‡æœ¬å…ƒç´ å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                text_elements.append(
                    cl.Text(
                        content=source_doc.page_content, name=source_name, display="side"
                    )
                )
            source_names = [text_el.name for text_el in text_elements]

            # å¦‚æœæœ‰æºæ–‡æ¡£åç§°ï¼Œå°†å…¶æ·»åŠ åˆ°å›ç­”ä¸­
            if source_names:
                answer += f"\næ¥æº: {', '.join(source_names)}"
            else:
                answer += "\næœªæ‰¾åˆ°æ¥æº"

        # å‘é€å›ç­”å’Œæºæ–‡æ¡£ç»™ç”¨æˆ·
        await cl.Message(content=answer, elements=text_elements).send()

    except Exception as e:
        error_msg = f"å¤±è´¥: {str(e)}"
        await cl.Message(content=error_msg).send()



